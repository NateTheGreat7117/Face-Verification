{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc3c737",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as alb\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import skimage\n",
    "import shutil\n",
    "import time\n",
    "import math\n",
    "import glob\n",
    "import json\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cff302",
   "metadata": {},
   "source": [
    "# Set GPU Growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b338c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid out of memory errors\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d3f79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e7ffc8",
   "metadata": {},
   "source": [
    "# Build Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b24cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(\"images/my_face/\")\n",
    "\n",
    "filenames = [files[i][:-4] for i in range(0, len(files), 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213d13c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(file):\n",
    "    byte_img = tf.io.read_file(file)\n",
    "    img = tf.io.decode_jpeg(byte_img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfe1319",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentor = alb.Compose([alb.HorizontalFlip(p=0.5), \n",
    "                         alb.RandomBrightnessContrast(p=0.3),\n",
    "                         alb.RandomGamma(p=0.3),\n",
    "                         alb.RGBShift(p=0.3),\n",
    "                         alb.VerticalFlip(p=0.5)], \n",
    "                       bbox_params=alb.BboxParams(format='albumentations', \n",
    "                                                  label_fields=['class_labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe184f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detection dataset\n",
    "filenames = os.listdir(\"images/detection/\")\n",
    "filenames = filenames + os.listdir(\"images/my_face\")\n",
    "filenames = [filenames[i][:-4] for i in range(0, len(filenames), 2)]\n",
    "\n",
    "faces = 0\n",
    "total = 0\n",
    "\n",
    "folder = \"none\"\n",
    "\n",
    "for file in filenames:\n",
    "    if os.path.exists(f\"images/detection/{file}.json\"):\n",
    "        folder = \"detection\"\n",
    "    elif os.path.exists(f\"images/my_face/{file}.json\"):\n",
    "        folder = \"my_face\"\n",
    "        \n",
    "    if folder != \"none\":\n",
    "        img = cv2.imread(os.path.join('images', folder, file + \".jpg\"))\n",
    "        \n",
    "        with open(f\"images/{folder}/\" + file + \".json\", \"r\") as f:\n",
    "            data = json.load(f)\n",
    "            class_id = 0\n",
    "            if len(data[\"shapes\"]) > 0:\n",
    "                class_id = 1\n",
    "                faces += 1\n",
    "            total += 1\n",
    "\n",
    "        for i in range(10):\n",
    "            augmented = augmentor(image=np.array(img), bboxes=[[0,0,0.00001,0.00001]], class_labels=[\"face\"])\n",
    "                \n",
    "            cv2.imwrite(os.path.join('images', 'augmented', 'detection_images', f'{file}.{i}.jpg'), augmented['image'])\n",
    "\n",
    "            annotation = {}\n",
    "            annotation['image'] = file\n",
    "\n",
    "            annotation['class'] = class_id\n",
    "\n",
    "            with open(os.path.join('images', 'augmented', 'detection_labels', f'{file}.{i}.json'), 'w') as f:\n",
    "                json.dump(annotation, f)\n",
    "    folder = \"none\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5523a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"True outputs: \", faces)\n",
    "print(\"False outputs: \", total - faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88a0105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location dataset\n",
    "filenames = os.listdir(\"images/my_face\")\n",
    "filenames = [filenames[i][:-4] for i in range(0, len(files), 2)]\n",
    "\n",
    "for file in filenames:\n",
    "    if os.path.exists(f\"images/my_face/{file}.json\"):\n",
    "        img = cv2.imread(os.path.join('images', 'my_face', file + \".jpg\"))\n",
    "        \n",
    "        with open(\"images/my_face/\" + file + \".json\", \"r\") as f:\n",
    "            data = json.load(f)\n",
    "            coordinates = [0,0,0.00001,0.00001]\n",
    "            if len(data[\"shapes\"]) > 0:\n",
    "                point = data[\"shapes\"][0][\"points\"]\n",
    "                coordinates[0] = point[0][0] / img.shape[1]\n",
    "                coordinates[1] = point[0][1] / img.shape[0]\n",
    "                coordinates[2] = point[1][0] / img.shape[1]\n",
    "                coordinates[3] = point[1][1] / img.shape[0]\n",
    "                faces += 1\n",
    "\n",
    "                for i in range(10):\n",
    "                    augmented = augmentor(image=np.array(img), bboxes=[coordinates], class_labels=[\"face\"])\n",
    "\n",
    "                    cv2.imwrite(os.path.join('images', 'augmented', 'detection_images', f'{file}.{i}.jpg'), augmented['image'])\n",
    "\n",
    "                    annotation = {}\n",
    "                    annotation['image'] = file\n",
    "\n",
    "\n",
    "                    cv2.imwrite(os.path.join('images', 'augmented', 'location_images', f'{file}.{i}.jpg'), augmented['image'])\n",
    "                    annotation['bbox'] = augmented['bboxes'][0]\n",
    "                    with open(os.path.join('images', 'augmented', 'location_labels', f'{file}.{i}.json'), 'w') as f:\n",
    "                        json.dump(annotation, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0be13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_images = tf.data.Dataset.list_files('images\\\\augmented\\\\detection_images\\\\*.jpg', shuffle=False)\n",
    "detection_images = detection_images.map(load_image)\n",
    "detection_images = detection_images.map(lambda x: tf.image.resize(x, (120,120)))\n",
    "detection_images = detection_images.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfcd577",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_images = tf.data.Dataset.list_files(\"images\\\\augmented\\\\location_images\\\\*.jpg\", shuffle=False)\n",
    "location_images = location_images.map(load_image)\n",
    "location_images = location_images.map(lambda x: tf.image.resize(x, (120, 120)))\n",
    "location_images = location_images.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b40acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_detection_labels(label_path):\n",
    "    with open(label_path.numpy(), 'r', encoding = \"utf-8\") as f:\n",
    "        label = json.load(f)\n",
    "        \n",
    "    return [label['class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5cf850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_location_labels(label_path):\n",
    "    with open(label_path.numpy(), 'r', encoding = \"utf-8\") as f:\n",
    "        label = json.load(f)\n",
    "        \n",
    "    return [label['bbox']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11d880c",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_labels = tf.data.Dataset.list_files('images\\\\augmented\\\\detection_labels\\\\*.json', shuffle=False)\n",
    "detection_labels = detection_labels.map(lambda x: tf.py_function(load_detection_labels, [x], [tf.uint8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da71607",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_labels = tf.data.Dataset.list_files(\"images\\\\augmented\\\\location_labels\\\\*.json\", shuffle=False)\n",
    "location_labels = location_labels.map(lambda x: tf.py_function(load_location_labels, [x], [tf.float16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3d1d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_data = tf.data.Dataset.zip((detection_images, detection_labels))\n",
    "detection_data = detection_data.shuffle(5000)\n",
    "detection_data = detection_data.batch(8)\n",
    "detection_data = detection_data.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b20ad1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_data = tf.data.Dataset.zip((location_images, location_labels))\n",
    "location_data = location_data.shuffle(5000)\n",
    "location_data = location_data.batch(8)\n",
    "location_data = location_data.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f37d0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = int(len(detection_data) * .8)\n",
    "VAL_SIZE = int((len(detection_data) - TRAIN_SIZE) / 2)\n",
    "\n",
    "detection_train = detection_data.take(TRAIN_SIZE)\n",
    "detection_val = detection_data.skip(TRAIN_SIZE).take(VAL_SIZE)\n",
    "detection_test = detection_data.skip(TRAIN_SIZE + VAL_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1caee2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = int(len(location_data) * .8)\n",
    "VAL_SIZE = int((len(location_data) - TRAIN_SIZE) / 2)\n",
    "\n",
    "location_train = location_data.take(TRAIN_SIZE)\n",
    "location_val = location_data.skip(TRAIN_SIZE).take(VAL_SIZE)\n",
    "location_test = location_data.skip(TRAIN_SIZE + VAL_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3e258f",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.random.randint(8)\n",
    "index = 6\n",
    "rand_batch = np.random.randint(33)\n",
    "for image, labels in location_train.skip(rand_batch).take(1):\n",
    "    plt.imshow(image[index])\n",
    "    \n",
    "    point = np.array(labels[0][index]) * 120\n",
    "\n",
    "    print(labels[0][index])\n",
    "    print(point)\n",
    "    plt.gca().add_patch(Rectangle((point[0],point[1]), point[2]-point[0], point[3]-point[1], \n",
    "                                  edgecolor=\"green\", facecolor=\"none\", lw=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a774fdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.random.randint(8)\n",
    "index = 6\n",
    "rand_batch = np.random.randint(162)\n",
    "for image, labels in detection_train.skip(rand_batch).take(1):\n",
    "    plt.imshow(image[index])\n",
    "    \n",
    "    class_id = labels[0][index]\n",
    "\n",
    "    print(class_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae056a4a",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fcae26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection_model():\n",
    "    input = tf.keras.Input(shape=(120, 120, 3))\n",
    "    \n",
    "    base_model = tf.keras.applications.VGG19(include_top=False, weights=\"imagenet\", input_shape=(120, 120, 3))\n",
    "#     base_model = tf.keras.applications.MobileNetV2(include_top=False, weights=\"imagenet\", input_shape=(120, 120, 3))\n",
    "    out = base_model(input)\n",
    "    \n",
    "    x = tf.keras.layers.GlobalMaxPooling2D()(out)\n",
    "    x = tf.keras.layers.Dense(256, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=input, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13aeeb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def location_model():\n",
    "    input = tf.keras.Input(shape=(120, 120, 3))\n",
    "    \n",
    "    base_model = tf.keras.applications.VGG19(include_top=False, weights=\"imagenet\", input_shape=(120, 120, 3))\n",
    "#     base_model = tf.keras.applications.MobileNetV2(include_top=False, weights=\"imagenet\", input_shape=(120, 120, 3))\n",
    "    out = base_model(input)\n",
    "    \n",
    "    x = tf.keras.layers.GlobalMaxPooling2D()(out)\n",
    "    x = tf.keras.layers.Dense(512, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Dense(4, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=input, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0332d610",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection = detection_model()\n",
    "location = location_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b49709e",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5729794",
   "metadata": {},
   "outputs": [],
   "source": [
    "location.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ec9bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def localization_loss(y_true, yhat):            \n",
    "    delta_coord = tf.reduce_sum(tf.square(y_true[:,:2] - yhat[:,:2]))\n",
    "                  \n",
    "    h_true = y_true[:,3] - y_true[:,1] \n",
    "    w_true = y_true[:,2] - y_true[:,0] \n",
    "\n",
    "    h_pred = yhat[:,3] - yhat[:,1] \n",
    "    w_pred = yhat[:,2] - yhat[:,0] \n",
    "    \n",
    "    delta_size = tf.reduce_sum(tf.square(w_true - w_pred) + tf.square(h_true-h_pred))\n",
    "    \n",
    "    return delta_coord + delta_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a231132",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy())\n",
    "location.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "                 loss=localization_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89df894b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "detection_history = detection.fit(detection_train, epochs=50, validation_data=detection_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27830b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = []\n",
    "predicted = []\n",
    "true_positive = 0\n",
    "true_negative = 0\n",
    "false_positive = 0\n",
    "false_negative = 0\n",
    "\n",
    "print(\"Calculating...\")\n",
    "for image, label in detection_test:\n",
    "    for batch in range(len(label)):\n",
    "        class_id = detection.predict(np.expand_dims(image[batch], axis=0), verbose=0)\n",
    "\n",
    "        actual.append(label[0][batch])\n",
    "        predicted.append(np.round(class_id[0][0]))\n",
    "\n",
    "        if np.round(class_id[0][0]) == label[0][batch]:\n",
    "            if class_id == 1:\n",
    "                true_positive += 1\n",
    "            else:\n",
    "                true_negative += 1\n",
    "        else:\n",
    "            if class_id == 1:\n",
    "                false_positive += 1\n",
    "            else:\n",
    "                false_negative += 1\n",
    "print(\"Done\")\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(actual, predicted)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=[\"Hand\", \"No hand\"])\n",
    "\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7011ed1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "location_history = location.fit(true_train, epochs=400, validation_data=true_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b647ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(20,5))\n",
    "\n",
    "ax[0].plot(detection_history.history['loss'], label='loss')\n",
    "ax[0].plot(detection_history.history['val_loss'], label='val_loss')\n",
    "ax[1].plot(location_history.history['loss'], label='loss')\n",
    "ax[1].plot(location_history.history['val_loss'], label='val_loss')\n",
    "\n",
    "plt.ylim([0, max(plt.ylim())])\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('CE/token')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf20629",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, label in detection_test.skip(1).take(1):\n",
    "    image = image[0]\n",
    "    plt.imshow(np.array(image))\n",
    "    \n",
    "    class_id = detection.predict(np.array([image]))\n",
    "    point = location.predict(np.array([image]))\n",
    "    point = point[0] * 120.0\n",
    "    \n",
    "    print(class_id[0][0])\n",
    "    print(point)\n",
    "    \n",
    "    if class_id[0][0] > .1:\n",
    "        plt.gca().add_patch(Rectangle((point[0], point[1]), point[2]-point[0], point[3]-point[1],\n",
    "                                     edgecolor=\"green\", facecolor=\"none\", lw=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7954cd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = load_image(\"images/Nathan1.jpeg\")\n",
    "shape = image.shape\n",
    "plt.imshow(np.array(image)/255.0)\n",
    "\n",
    "image = np.array([tf.image.resize(image, (120, 120))])/255.0\n",
    "\n",
    "start = time.time()\n",
    "class_id = detection.predict(image)\n",
    "point = location.predict(image)\n",
    "print(time.time() - start)\n",
    "point = point[0] * [shape[1], shape[0], shape[1], shape[0]]\n",
    "\n",
    "print(class_id[0][0])\n",
    "print(point)\n",
    "\n",
    "if class_id[0][0] > 0:\n",
    "    plt.gca().add_patch(Rectangle((point[0],point[1]), point[2]-point[0], point[3]-point[1], \n",
    "                                  edgecolor=\"green\", facecolor=\"none\", lw=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45156b65",
   "metadata": {},
   "source": [
    "# Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ee1e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection.save(\"face_detection\")\n",
    "location.save(\"face_location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574ae3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection = tf.keras.models.load_model(\"face_detection\")\n",
    "location = tf.keras.models.load_model(\"face_location\", compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a237c68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    _, frame = cap.read()\n",
    "\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img = tf.image.resize(rgb, (120, 120))\n",
    "    class_id = detection.predict(np.array([img])/255.0, verbose=0)\n",
    "    point = location.predict(np.array([img])/255.0, verbose=0)\n",
    "    point = point[0] * [640, 480, 640, 480]\n",
    "\n",
    "    if class_id[0][0] > 0.5:\n",
    "        frame = cv2.putText(frame, f\"Score: {class_id[0][0]}\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "        frame = cv2.rectangle(frame, (int(point[0]), int(point[1])), \n",
    "                            (int(point[2]), int(point[3])), color=(0, 255, 0))\n",
    "    \n",
    "    print(frame.shape)\n",
    "    cv2.imshow(\"Window\", frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5b03eb",
   "metadata": {},
   "source": [
    "# Convert to TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1ed4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def representative_dataset():\n",
    "    for _ in range(100):\n",
    "        data = np.random.rand(1, 120, 120, 3)\n",
    "        yield [data.astype(np.float16)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fe8ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"face_detection\")\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(\"face_detection.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d73864",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"face_location\")\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(\"face_location.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a330c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_interpreter = tf.lite.Interpreter(\"face_detection.tflite\")\n",
    "detection_signature = detection_interpreter.get_signature_runner()\n",
    "\n",
    "location_interpreter = tf.lite.Interpreter(\"face_location.tflite\")\n",
    "location_signature = location_interpreter.get_signature_runner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676d6860",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = load_image(\"images/Nathan1.jpeg\")\n",
    "shape = image.shape\n",
    "plt.imshow(np.array(image) / 255.0)\n",
    "\n",
    "image = tf.image.resize(image, (120, 120)) / 255.0\n",
    "image = np.array([image])\n",
    "\n",
    "start = time.time()\n",
    "class_id = detection_signature(input_43=tf.cast(image, tf.float32))\n",
    "class_id = class_id[\"dense_41\"]\n",
    "coordinates = location_signature(input_45=tf.cast(image, tf.float32))\n",
    "coordinates = coordinates[\"dense_43\"]\n",
    "print(\"Time taken: \", time.time() - start)\n",
    "\n",
    "point = coordinates[0] * [shape[1], shape[0], shape[1], shape[0]]\n",
    "\n",
    "print(class_id[0][0])\n",
    "print(point)\n",
    "\n",
    "if class_id[0][0] > 0:\n",
    "    plt.gca().add_patch(Rectangle((point[0],point[1]), point[2]-point[0], point[3]-point[1], \n",
    "                                  edgecolor=\"green\", facecolor=\"none\", lw=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c6cb2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
